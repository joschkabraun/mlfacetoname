{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from keras_vggface.utils import decode_predictions\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a single face from a given photograph\n",
    "def extract_face(filename, required_size=(224, 224)):\n",
    "    # load image from file\n",
    "    pixels = pyplot.imread(filename)\n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    results = detector.detect_faces(pixels)\n",
    "    # extract the bounding box from the first face\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "    return face_array\n",
    "\n",
    "# extract faces and calculate face embeddings for a list of photo files\n",
    "def get_embeddings(filenames):\n",
    "    faces = []\n",
    "    \n",
    "    # extract faces\n",
    "    for f_name in filenames:\n",
    "        try:\n",
    "            faces.append(extract_face(f_name))\n",
    "        except ValueError:\n",
    "            print(f_name)\n",
    "    \n",
    "    # convert into an array of samples\n",
    "    samples = asarray(faces, 'float32')\n",
    "    # prepare the face for the model, e.g. center pixels\n",
    "    samples = preprocess_input(samples, version=2)\n",
    "    # create a vggface model\n",
    "    model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "    # perform prediction\n",
    "    yhat = model.predict(samples)\n",
    "    return yhat\n",
    "\n",
    "# get all the paths of all files (including subdirectories) of given format\n",
    "def get_paths_of_files_of_format(path, file_format=\"jpg\"):\n",
    "    files = []\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if file_format in file:\n",
    "                files.append(os.path.join(r, file))\n",
    "    \n",
    "    return files\n",
    "\n",
    "# extract names of the images in the used files and composes them as (first name) (given name) (i.e. with a space)\n",
    "def extract_full_names(path_files,dataset=\"lfw\"):\n",
    "    names = []\n",
    "    for f in path_files:\n",
    "        fname = f.split(\"\\\\\")[-1]              # the files always were separated by \"\\\\\" in the end\n",
    "        \n",
    "        if dataset==\"lfw\":\n",
    "            fname = fname.split(\"_\")\n",
    "            names.append(fname[0]+\" \"+fname[1])\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only code for the lfw dataset have been implemented\")\n",
    "    return names\n",
    "\n",
    "# averages vectors to associated name; expects numpy vectors as input\n",
    "class NamedAvgVector:\n",
    "    def __init__(self, name, numpy_vec=\"a\"):\n",
    "        self.name = name\n",
    "        self.vec = numpy_vec\n",
    "        if numpy_vec != \"a\":\n",
    "            self.n = 1\n",
    "        else:\n",
    "            self.n = 0\n",
    "    \n",
    "    def update(self, numpy_vec):\n",
    "        if self.n != 0:\n",
    "            self.vec *= self.n\n",
    "            self.vec += numpy_vec\n",
    "            self.n += 1\n",
    "            self.vec /= self.n\n",
    "        else:\n",
    "            self.n = 1\n",
    "            self.vec = numpy_vec\n",
    "            \n",
    "# creates a dictionary with all the first names as keys and associated empty NamedAvgVector data structures\n",
    "def create_initial_dict(full_names):\n",
    "    dict_list = []\n",
    "    for name in full_names:\n",
    "        first_name = name.split(\" \")[0]\n",
    "        dict_list.append((first_name, NamedAvgVector(first_name)))\n",
    "    return dict(dict_list)\n",
    "\n",
    "# updates the dictionary of NamedAvgVectors with the embeddings\n",
    "# assumes that all embeddings are already in dictionary\n",
    "# i_0_f_names : is the starting point of the calculated embeddings in full_names\n",
    "def update_dict(d, full_names, i_0_f_names, emb):\n",
    "    for i in range(len(emb)):\n",
    "        fname = full_names[i_0_f_names + i].split(\" \")[0]\n",
    "        d[fname].update(np.array(emb[i]))\n",
    "        \n",
    "# calculates the averaged embeddings for all the first names given a dataset\n",
    "def calculate_embeddings(path_dataset, step_size=100, file_format=\"jpg\", dataset_name=\"lfw\"):\n",
    "    im_paths = get_paths_of_files_of_format(path_lfw, file_format)\n",
    "    size = len(im_paths)\n",
    "    full_names = extract_full_names(im_paths)\n",
    "    dict_navecs = create_initial_dict(full_names)\n",
    "    \n",
    "    for i in range(math.floor(size/step_size)):\n",
    "        print(\"calculate_embeddings i:\", i)\n",
    "        im_paths_tmp = im_paths[i*step_size : (i+1)*step_size]\n",
    "        emb_tmp = get_embeddings(im_paths_tmp)\n",
    "        update_dict(dict_navecs, full_names, i*step_size, emb_tmp)\n",
    "        \n",
    "        # save progress so far\n",
    "        f = open(\"dict_{}.pkl\".format(i), \"wb\")\n",
    "        pickle.dump(dict_navecs, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# returns the maximal coordinate value of the embeddings in the dictionary and prints the number of entries w/o any value\n",
    "def get_max_coor_val(dic):\n",
    "    m=0                       # weak\n",
    "    c=0\n",
    "    for key in dic.keys():\n",
    "        if isinstance(dic[key].vec, int):\n",
    "            print(\"{} : int\".format(key))\n",
    "            c+=1\n",
    "            continue\n",
    "        elif isinstance(dic[key].vec, str):\n",
    "            print(\"{} : str\".format(key))\n",
    "            c+=1\n",
    "            continue\n",
    "        m=max(m,max(dic[key].vec))\n",
    "    print(\"no entry #\",c)\n",
    "    return m\n",
    "\n",
    "# returns the average number of used embeddings per name\n",
    "def get_avg_n(dic):\n",
    "    accum = 0\n",
    "    cnt = 0\n",
    "    for val in dic.values():\n",
    "        if isinstance(val.vec, int) or isinstance(val.vec, str):\n",
    "            continue\n",
    "        cnt +=1\n",
    "        accum += val.n\n",
    "    return accum/cnt\n",
    "\n",
    "# returns the percentage of entries in the dictionary which used less than n embeddings\n",
    "def perct_less_than(dic,n):\n",
    "    cnt = 0\n",
    "    for val in dic.values():\n",
    "        if isinstance(val.vec, int) or isinstance(val.vec, str) or val.n < n:\n",
    "            cnt += 1\n",
    "    return cnt / len(dic.values())\n",
    "\n",
    "# prints the keys in the dictionary which used more than n embeddings\n",
    "def names_more_than(dic,n):\n",
    "    for key in dic.keys():\n",
    "        val = dic[key]\n",
    "        if isinstance(val.vec, int) or isinstance(val.vec, str) or val.n <= n:\n",
    "            continue\n",
    "        else:\n",
    "            print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the closest name in dictionary given an embedding\n",
    "def find_closest_name(dic, vec_emb_numpy):\n",
    "    d_distances_l = []\n",
    "    for key in dic.keys():\n",
    "        val = d_num[key]\n",
    "        if isinstance(val.vec, int) or isinstance(val.vec, str):        # if no value has been assigned\n",
    "            continue\n",
    "        dist = np.linalg.norm(val.vec-vec_emb_numpy)\n",
    "        d_distances_l.append((dist,key))\n",
    "    d_distances = dict(d_distances_l)\n",
    "    return d_distances[min(d_distances.keys())]\n",
    "\n",
    "# predicts a name for each file provided in the given list based on the provided dictionary\n",
    "def predict_name(dic, filenames):\n",
    "    embeddings = get_embeddings(filenames)\n",
    "    name_predictions = []\n",
    "    for vec in embeddings:\n",
    "        name_predictions.append(find_closest_name(dic,np.array(vec)))\n",
    "    return name_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"dict_lfw.pkl\".format(num), \"rb\")\n",
    "dic = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_wrong = 0\n",
    "for key in dic.keys():\n",
    "    val = dic[key]\n",
    "    if isinstance(val.vec, int) or isinstance(val.vec, str):        # if no value has been assigned\n",
    "        continue\n",
    "    if key != find_next_name(dic, val.vec):\n",
    "        cnt_wrong += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(cnt_wrong)                # --> 0; i.e. model predicts perfectly name of persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []                    # add file names\n",
    "predictions = predict_name(dic, filenames)\n",
    "for name in predictions:\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
