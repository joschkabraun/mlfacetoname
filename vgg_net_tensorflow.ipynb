{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from keras_vggface.utils import decode_predictions\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamedAvgVector:\n",
    "    \"\"\"A class used to combine a name, a vector (the embedding) and the number of used vectors in one data structure.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    name : str\n",
    "        The name\n",
    "    vec : numpy array of int\n",
    "        The average vector\n",
    "    n : int\n",
    "        The number of used vectors\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    update(numpy_vec)\n",
    "        Updates the average vector and used vectors with the given numpy vector\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, numpy_vec=\"a\"):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            The name\n",
    "        numpy_vec : numpy vector of int, optional\n",
    "            The initialized average vector (default is \"a\" to declare it is unitialized)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.name = name\n",
    "        self.vec = numpy_vec\n",
    "        if numpy_vec != \"a\":\n",
    "            self.n = 1\n",
    "        else:\n",
    "            self.n = 0\n",
    "    \n",
    "    def update(self, numpy_vec):\n",
    "        \"\"\"Updates the average vector and used vectors with the given numpy vector\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        numpy_vec : numpy array of int\n",
    "            The vector with which the average vector shall be updated\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.n != 0:\n",
    "            self.vec *= self.n\n",
    "            self.vec += numpy_vec\n",
    "            self.n += 1\n",
    "            self.vec /= self.n\n",
    "        else:\n",
    "            self.n = 1\n",
    "            self.vec = numpy_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face(filename, required_size=(224, 224)):\n",
    "    \"\"\"Extracts a single face from a given photograph using MTCNN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The filename of which the face shall be extracted\n",
    "    required_size : touple of int, optional\n",
    "        The size which the extracted face shall have (default is (224, 224))\n",
    "    \"\"\"\n",
    "    \n",
    "    # load image from file\n",
    "    pixels = pyplot.imread(filename)\n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    results = detector.detect_faces(pixels)\n",
    "    # extract the bounding box from the first face\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "    return face_array\n",
    "\n",
    "def get_embeddings(filenames):\n",
    "    \"\"\"Given a list of file names it extracts first the face and then computes their embeddings.\n",
    "\n",
    "    This method uses VGGFace resnet50 model (which version of VGGNet?)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filenames : list of str\n",
    "        The list of file names of which embeddings shall be calculated\n",
    "    \"\"\"\n",
    "    \n",
    "    faces = []\n",
    "    \n",
    "    # extract faces\n",
    "    for f_name in filenames:\n",
    "        try:\n",
    "            faces.append(extract_face(f_name))\n",
    "        except ValueError:\n",
    "            print(f_name)\n",
    "    \n",
    "    # convert into an array of samples\n",
    "    samples = asarray(faces, 'float32')\n",
    "    # prepare the face for the model, e.g. center pixels\n",
    "    samples = preprocess_input(samples, version=2)\n",
    "    # create a vggface model\n",
    "    model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "    # perform prediction\n",
    "    yhat = model.predict(samples)\n",
    "    return yhat\n",
    "\n",
    "def get_paths_of_files_of_format(path, file_format=\"jpg\"):\n",
    "    \"\"\"Finds all the paths of files with specified file format in all subdirectories of the specified path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        The path of the starting directory\n",
    "    file_format : str, optional\n",
    "        The file format of which the all files shall be found (default is \"jpg\")\n",
    "    \"\"\"\n",
    "    \n",
    "    files = []\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if file_format in file:\n",
    "                files.append(os.path.join(r, file))\n",
    "    \n",
    "    return files\n",
    "\n",
    "def extract_full_names(path_files,dataset=\"lfw\"):\n",
    "    \"\"\"Extract the full names of the given files.\n",
    "\n",
    "    This function was specifically written for the Labeled Faces of the Wild Dataset (LFW).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_files : str\n",
    "        The file paths of which the full names shall be extracted\n",
    "    dataset : str, optional\n",
    "        The dataset which is examined (default is \"lfw\")\n",
    "    \"\"\"\n",
    "    \n",
    "    names = []\n",
    "    for f in path_files:\n",
    "        fname = f.split(\"\\\\\")[-1]              # the files always were separated by \"\\\\\" in the end\n",
    "        \n",
    "        if dataset==\"lfw\":\n",
    "            fname = fname.split(\"_\")\n",
    "            names.append(fname[0]+\" \"+fname[1])\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only code for the lfw dataset have been implemented\")\n",
    "    return names\n",
    "            \n",
    "def create_initial_dict(full_names):\n",
    "    \"\"\"Returns a dictionary of NamedAvgVector of the first name for the given names with unitialized average vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    full_names : list of str\n",
    "        The list of full names for which a dictionary of NamedAvgVector shall be created\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_list = []\n",
    "    for name in full_names:\n",
    "        first_name = name.split(\" \")[0]\n",
    "        dict_list.append((first_name, NamedAvgVector(first_name)))\n",
    "    return dict(dict_list)\n",
    "\n",
    "def update_dict(d, full_names, i_0_f_names, emb):\n",
    "    \"\"\"Updates the dictionary of NamedAvgVector at the specified position with the given embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : dict of NamedAvgVector\n",
    "        The to be updated dictionary of NamedAvgVector\n",
    "    full_names : list of str\n",
    "        The list of full names used in the dictionary (possibly have repetitions)\n",
    "    i_0_f_names : int\n",
    "        The starting index in full_names where the embeddings were calculated\n",
    "    emb : list of numpy array of int\n",
    "        The calculated embeddings which shall be used for the update\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(emb)):\n",
    "        fname = full_names[i_0_f_names + i].split(\" \")[0]\n",
    "        d[fname].update(np.array(emb[i]))\n",
    "        \n",
    "def calculate_embeddings(path_dataset, step_size=100, file_format=\"jpg\", dataset_name=\"lfw\"):\n",
    "    \"\"\"Calculates the averaged embeddings for all the first names given a dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_dataset : str\n",
    "        The path of the dataset\n",
    "    step_size : int, optional\n",
    "        The number of embeddings which shall be calculated for every update (default is 100)\n",
    "    file_format : str, optional\n",
    "        The used file format of the images (default is \"jpg\")\n",
    "    dataset_name : str, optional\n",
    "        The underlying used data set (default is \"lfw\")\n",
    "    \"\"\"\n",
    "    \n",
    "    im_paths = get_paths_of_files_of_format(path_lfw, file_format)\n",
    "    size = len(im_paths)\n",
    "    full_names = extract_full_names(im_paths)\n",
    "\n",
    "    dict_navecs = create_initial_dict(full_names)\n",
    "    \n",
    "    for i in range(math.ceil(size/step_size)):\n",
    "        print(\"calculate_embeddings i:\", i)\n",
    "        im_paths_tmp = im_paths[i*step_size : (i+1)*step_size]\n",
    "        emb_tmp = get_embeddings(im_paths_tmp)\n",
    "        update_dict(dict_navecs, full_names, i*step_size, emb_tmp)\n",
    "        \n",
    "        # save progress so far\n",
    "        f = open(\"dict_{}.pkl\".format(i), \"wb\")\n",
    "        pickle.dump(dict_navecs, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# returns the maximal coordinate value of the embeddings in the dictionary and prints the number of entries w/o any value\n",
    "def get_max_coor_val(dic):\n",
    "    m=0                       # weak\n",
    "    c=0\n",
    "    for key in dic.keys():\n",
    "        if isinstance(dic[key].vec, int):\n",
    "            print(\"{} : int\".format(key))\n",
    "            c+=1\n",
    "            continue\n",
    "        elif isinstance(dic[key].vec, str):\n",
    "            print(\"{} : str\".format(key))\n",
    "            c+=1\n",
    "            continue\n",
    "        m=max(m,max(dic[key].vec))\n",
    "    print(\"no entry #\",c)\n",
    "    return m\n",
    "\n",
    "# returns the average number of used embeddings per name\n",
    "def get_avg_n(dic):\n",
    "    accum = 0\n",
    "    cnt = 0\n",
    "    for val in dic.values():\n",
    "        if isinstance(val.vec, int) or isinstance(val.vec, str):\n",
    "            continue\n",
    "        cnt +=1\n",
    "        accum += val.n\n",
    "    return accum/cnt\n",
    "\n",
    "# returns the percentage of entries in the dictionary which used less than n embeddings\n",
    "def perct_less_than(dic,n):\n",
    "    cnt = 0\n",
    "    for val in dic.values():\n",
    "        if isinstance(val.vec, int) or isinstance(val.vec, str) or val.n < n:\n",
    "            cnt += 1\n",
    "    return cnt / len(dic.values())\n",
    "\n",
    "# prints the keys in the dictionary which used more than n embeddings\n",
    "def names_more_than(dic,n):\n",
    "    for key in dic.keys():\n",
    "        val = dic[key]\n",
    "        if isinstance(val.vec, int) or isinstance(val.vec, str) or val.n <= n:\n",
    "            continue\n",
    "        else:\n",
    "            print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2' \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "def find_closest_name(dic, vec_emb_numpy, distance_metric=\"norm\"):\n",
    "    \"\"\"Finds the name in dic with the closest vector embedding to vec_emb_numpy\n",
    "    \n",
    "    Implemented distance metrices are \"norm\" and \"angle\". The option \"norm\" has been better in performance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dic : dict of NamedAvgVector objects\n",
    "        The dictionary with the averaged vectors for a first name\n",
    "    vec_emb_numpy : numpy array of int\n",
    "        The vector embedding of whom the closest first name shall be found\n",
    "    distance_metric : str, optional\n",
    "        The used distance metric to determine which is the closest\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        If the desired distance_metric is not implemented\n",
    "    \"\"\"\n",
    "    \n",
    "    if distance_metric != \"norm\" and distance_metric != \"angle\":\n",
    "        raise NotImplementedError(\"Only metrices for norm and angle are implemented\")\n",
    "    \n",
    "    d_distances_l = []\n",
    "    for key in dic.keys():\n",
    "        val = dic[key]\n",
    "        if isinstance(val.vec, int) or isinstance(val.vec, str):        # if no value has been assigned\n",
    "            continue\n",
    "            \n",
    "        dist = 0\n",
    "        if distance_metric == \"norm\":\n",
    "            dist = np.linalg.norm(val.vec-vec_emb_numpy)\n",
    "        elif distance_metric == \"angle\":\n",
    "            dist = angle_between(val.vec,vec_emb_numpy)\n",
    "        \n",
    "        d_distances_l.append((dist,key))\n",
    "    d_distances = dict(d_distances_l)\n",
    "    return d_distances[min(d_distances.keys())]\n",
    "\n",
    "def predict_name(dic, filenames, distance_metric=\"norm\"):\n",
    "    \"\"\"Based on the given dictionary and distance metric, predictions for the first name of the person in the given files are made.\n",
    "    \n",
    "    Implemented distance metrices are \"norm\" and \"angle\". The option \"norm\" has been better in performance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dic : dict of NamedAvgVector objects\n",
    "        The dictionary with the averaged vectors for a first name\n",
    "    filenames : list of str\n",
    "        The name of the files for which picture the prediction shall be made\n",
    "    distance_metric : str, optional\n",
    "        The used distance metric to determine which is the closest\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = get_embeddings(filenames)\n",
    "    name_predictions = []\n",
    "    for vec in embeddings:\n",
    "        name_predictions.append(find_closest_name(dic,np.array(vec), distance_metric))\n",
    "    return name_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"dict_lfw.pkl\", \"rb\")\n",
    "dic = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []                        # fill in paths to files with whose first name shall be predicted\n",
    "pred = predict_name(dic, filenames)\n",
    "for name in pred:\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
